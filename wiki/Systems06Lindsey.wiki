#summary Lindsey studies for the systems qual.

= One: Threads =
== Which of the following items are typically considered to be unique to a process, but not unique to a thread? ==
  * CPU registers
  * page table pointer
  * stack pointer
  * open files
  * segment table
  * child processes
  * program counter

== Answer ==

Here are my off-the-top-of-my-head answers:

CPU registers: I think these are unique to a thread; they're part of the context that gets saved when we swap a thread out.

Page table pointer: Every process gets its own page table, yeah?  So the page table pointer is unique to a process, but not to a thread, since threads within a process have to share the memory that's allocated to that process.

Stack pointer and program counter: I think these would have to be unique to a thread.  Don't threads have to keep track of where they are in execution?

Open files: I can imagine two threads in a process having the same file open for reading and writing.  Unique to a process, not a thread.

Segment table: This sounds like something that keeps track of allocated and available memory?  It sounds like an OS-wide thing, not something unique to a process or a thread.

Child processes: An individual thread can't fork a child process; it has to be done at the process level.  So, unique to a process, not a thread.

So that leaves us with these items being unique to a process and not to a thread: page table pointer, open files, child processes.  Let's see what Tanenbaum says...

OK.  On page 102 of Modern Operating Systems 3e, we have:

|| *Per process items*         || *Per thread items* ||
|| Address space               || Program counter    ||
|| Global variables            || Registers          ||
|| Open files                  || Stack              ||
|| Child processes             || State              ||
|| Pending alarms              ||                    ||
|| Signals and signal handlers ||                    ||
|| Accounting information      ||                    ||

So I'm right about CPU registers, stack pointer (I'm assuming that's part of "stack"), open files, child processes, and program counter.  I don't know if I'm right about page table pointer and segment table.  Let's see what those are...

Hm.  The Internet suggests that I guessed wrong about segment tables and that a segment table keeps track of every segment (code, static data, heap, stack) in a particular process.  So that would be a process-wide, not unique to a thread, notion.

And for the page table pointer: I guess I was confusing page table with segment table.  The Internet would also seem to to suggest that there's only one, system-wide page table (and now that I think about it, that makes sense; at the page table level there's no notion of threads).  Furthermore, I guess the page table pointer is merely a pointer to the base of that single, system-wide page table.  So it's not unique to a process or a thread; it's OS-wide.

== Which of the following are true statements about user-level threads versus kernel-level threads? ==
  * Invoking any system call that might block poses an extra problem for user-level threads
  * User level threads are more vulnerable to priority inversion than kernel-level threads
  * There is less overall scheduling overhead in a user-level thread system
  * User level threads retain advantages on symmetric multiprocessors
  * Upcalls are necessary to implement user-level threads. (An upcall is a notification, provided by the kernel to the user-level thread run-time system, that a thread has blocked.)

== Answer == 

From Tanenbaum, p. 108 and thereabouts:

"Invoking any system call that might block poses an extra problem for user-level threads" -- true.  If there are several user-level threads in a process and one blocks on a system call, none of them can run because as far as the kernel is concerned, the entire process is blocking.  If the kernel is aware of threads it might be able to allow those threads not blocking on the system call to run.

"User level threads are more vulnerable to priority inversion than kernel-level threads" -- false.  I don't see any particular reason why this would be the case.

"There is less overall scheduling overhead in a user-level thread system" -- true, because there is no need to go into the kernel when a thread is finished running in order to pick another thread to run; thread scheduling and context switching can happen without calling int the kernel. 

"User level threads retain advantages on symmetric multiprocessors" -- false; I don't see any particular reason why this would be the case, either.

"Upcalls are necessary to implement user-level threads." -- I would say false, that upcalls are not _required_, although they might help with scheduling.

= Two: CPU Efficiency =
Measurements of a certain system have shown that the average process runs for a time T before blocking on I/O. A process switch requires time S, which is effectively wasted (over-head). For round-robin scheduling with quantum Q, give a formula for the CPU efficiency for each of the following.
  * Q = infinity
  * Q > T
  * S < Q < T
  * Q = S
  * Q nearly 0

== Answer ==

Off the top of my head:

Is the "CPU efficiency" the percentage of time spent actually computing something?  Let's assume that that's the case.

"round-robin scheduling with quantum Q" means that each process runs for time Q before another one is switched to.  I don't know if we should include the context switch time S in Q, or if it's in addition to Q.  Let's assume that it's included in Q.

Q = infinity: If Q is infinity, then the first process to use the CPU has it forever.  It runs for T of that time, then blocks (forever?), so the CPU usage is T/Q, or 0% in the limit!

Q > T: If Q is greater than T, then that means the first process to run runs for T, then nothing happens on the processor for Q-T time because the process is blocking.  Then, we switch to the next process, and the same thing happens.  This doesn't take into account S, which we have no information about.  If S is less than or equal to Q-T, then it can happen at the beginning of the next process' running time and still leave enough time for the process to run for T.  If that is the case, then the CPU usage is T/Q.  If S is more than Q-T, then the context switch time will cut into efficiency: we'll have to subtract S-(Q-T) from T.  So, (T - (S - (Q-T)))/Q.

S < Q < T: We know that S is less than Q, which is less than T.  Since Q is less than T, a process can't do all of its computation before blocking in the time that's given to it.  So it will run for all of Q, so the CPU will have 100% usage for that time; then, in the next quantum, there will be a context switch, taking time S, followed by a process running for Q-S time; then the same in the next quantum, and so on.  So, Q-S / Q.

Q = S: After the first process runs, the CPU won't have time to do anything but context switch, and nothing will have time during a quantum to actually start running.  So, CPU usage is 0% in the limit.

Q nearly 0: Again, there will be no time even to accomplish the context switching, so CPU usage is 0% in the limit.

= Three: Semaphores =
Suppose we have two threads Ta and Tb that we would like to wait for each other at a certain point in their execution

=== Thread Ta ===
{{{
statement a1
rendezvous_a()
statement a2
}}}
=== Thread Tb ===
{{{
statement b1
rendezvous_b()
statement b2
}}}

Here, we want Ta not to execute a2 until Tb has executed b1 and we want Tb not to execute b2 until Ta has executed a1. That is, we want the threads to "rendezvous" by calling rendezvous_a() and rendezvous_b().

Give a semaphore-based solution for rendezvous_a() and rendezvous_b(). Declare any variables you might need, along with their initial values. Make sure that deadlock is not possible in your solution.

== Answer ==

So, just to be clear, these orders of statements are OK:

a1, b1, a2, b2

b1, a1, a2, b2

a1, b1, b2, a2

b1, a1, b2, a2

But these aren't:

a1, a2, b1, b2

a1, a2, b2, b1

b1, b2, a1, a2

b1, b2, a2, a1

a1 and b1 both have to finish before either a2 or b2 may run.  I think we should be able to do this:

{{{
int sem = 2;

void rendezvous_a() {
sem--;
while(sem > 0) { wait(); }
}

void rendezvous_b() {
sem--;
while(sem > 0) { wait(); }
}
}}}

where wait() is just a function that sleeps that thread for an appropriate amount of time.

Off the top of my head, I don't see deadlock in this.  Neither rendezvous_ function can finish unless the other one is called, and that can't happen unless we're past the statement we cared about having happen.  One concern might be that the wait()ing doesn't allow enough time for the other thread to run, so we'd have to make sure to have the waiting period be long enough.