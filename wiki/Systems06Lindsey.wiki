#summary Lindsey studies for the systems qual.

= One: Threads =
== Which of the following items are typically considered to be unique to a process, but not unique to a thread? ==
  * CPU registers
  * page table pointer
  * stack pointer
  * open files
  * segment table
  * child processes
  * program counter

== Answer ==

CPU registers: These are unique to a thread; they're part of the context that gets saved when we swap a thread out.

Page table pointer: There is only one, OS-wide page table, so it wouldn't make sense for each thread to have a pointer into the page table; this would be a process-level notion if not an OS-level notion.

Stack pointer and program counter: These would have to be unique to a thread; threads have to keep track of where they are in execution.

Open files: Unique to a process, not a thread; two threads in a process might both access a file, even though only one of them opened it.

Segment table: This keeps track of allocated and available memory in a process' address space (for instance, the information in /proc/pid/maps for a particular pid on Linux), so it's process-specific, not thread-specific.

Child processes: An individual thread can't fork a child process; it has to be done at the process level.  So, unique to a process, not a thread.

So, in all, we've got:

  * CPU registers -- thread
  * page table pointer -- OS
  * stack pointer -- thread
  * open files -- process
  * segment table -- process
  * child processes -- process
  * program counter -- thread

On page 102 of Modern Operating Systems 3e, we have:

|| *Per process items*         || *Per thread items* ||
|| Address space               || Program counter    ||
|| Global variables            || Registers          ||
|| Open files                  || Stack              ||
|| Child processes             || State              ||
|| Pending alarms              ||                    ||
|| Signals and signal handlers ||                    ||
|| Accounting information      ||                    ||

I'm right about open files, child processes, program counter, CPU registers, stack pointer (I assume that is part of "stack").  "Address space" could possibly count as "segment table", so that's OK too.  No word about "page table pointer" -- I still think that that's OS-wide.

== Which of the following are true statements about user-level threads versus kernel-level threads? ==

  * Invoking any system call that might block poses an extra problem for user-level threads
  * User level threads are more vulnerable to priority inversion than kernel-level threads
  * There is less overall scheduling overhead in a user-level thread system
  * User level threads retain advantages on symmetric multiprocessors
  * Upcalls are necessary to implement user-level threads. (An upcall is a notification, provided by the kernel to the user-level thread run-time system, that a thread has blocked.)

== Answer == 

From Tanenbaum, p. 108 and thereabouts:

"Invoking any system call that might block poses an extra problem for user-level threads" -- true.  If there are several user-level threads in a process and one blocks on a system call, none of them can run because as far as the kernel is concerned, the entire process is blocking.  If the kernel is aware of threads it might be able to allow those threads not blocking on the system call to run.

"User level threads are more vulnerable to priority inversion than kernel-level threads" -- false.  I don't see any particular reason why this would be the case.

"There is less overall scheduling overhead in a user-level thread system" -- true, because there is no need to go into the kernel when a thread is finished running in order to pick another thread to run; thread scheduling and context switching can happen without calling int the kernel. 

"User level threads retain advantages on symmetric multiprocessors" -- false; I don't see any particular reason why this would be the case, either.

"Upcalls are necessary to implement user-level threads." -- I would say false, that upcalls are not _required_, although they might help with scheduling.

= Two: CPU Efficiency =
Measurements of a certain system have shown that the average process runs for a time T before blocking on I/O. A process switch requires time S, which is effectively wasted (over-head). For round-robin scheduling with quantum Q, give a formula for the CPU efficiency for each of the following.
  * Q = infinity
  * Q > T
  * S < Q < T
  * Q = S
  * Q nearly 0

== Answer ==

Is the "CPU efficiency" the percentage of time spent actually computing something?  Let's assume that that's the case.

Off the top of my head, I thought that round-robin scheduling with quantum Q meant that each process got time Q, whether it needed it or not; if it finished during its quantum, the CPU would be idle until the end of the quantum.  Some googling (this question shows up [http://www.google.com/search?q=Measurement+of+a+certain+system+have+shown+that+the+average+process+runs+for+a+time+T+before+blocking+on+IO. all over]) suggests that that's not the case, and Tanenbaum (p. 154) confirms it: "Each process is assigned a time interval, called its quantum, during which it is allowed to run.  If the process is still running at the end of the quantum, the CPU is preempted and given to another process.  If the process has blocked or finished before the quantum has elapsed, the CPU switching is done when the process blocks, of course."

I don't know if we should include the context switch time S in Q, or if it's in addition to Q, but let's assume that it's included in Q.

Q = infinity: The process will have the CPU until it blocks.  So, the process will run for T; then the next process will run for T (plus the S needed for the switch), and so on.  Each process will need T+S of processor time to get T worth of work done, so the efficiency is T/(T+S).  (Pretty good for CPU efficiency, but CPU efficiency may not be the only thing we want to maximize!)

Q > T: If the quantum is large enough that processes always block on I/O before it is over, the efficiency will be the same as in the "infinity" example: T/(T+S).

S < Q < T: Here, processes don't finish running during their quantum.  We don't know how big of a difference there is between Q and T, but we know that the process will require T/Q process switches during its lifetime, and each of those will take time S, so, S `*` (T/Q) time will be needed for context switching.  In all, we will need T + (S `*` (T/Q)) time to get work done that should take time T if it were allowed to happen all at once, so the efficiency is: T / (T + (ST/Q)).

Some clever people have simplified this as follows:

{{{
T            T              T              T              1          Q
--------  =  ----------- =  ----------- =  ----------- =  ------- =  ---
T + ST/Q     TQ/Q + ST/Q    (TQ + ST)/Q    T * (Q+S)/Q    (Q+S)/Q    Q+S
}}}

So, Q/(Q+S).

Q = S: Again, a process will need to do T/Q context switches in its lifetime, and if Q is S, that's equal to T/S.  It'll need to do that S times.  So, a total of T time spent context switching.  In all, we'll need T time to do work plus T time spent context switching, so, T /(T+T), or 50% efficiency.  (The internet agrees with this, but I don't buy it -- if Q = S, then we'll never have time to do _anything_ but context switch, so we won't get any actual work done.  Someone want to convince me otherwise?)

Q = nearly 0: We won't even have time to do the context switching -- efficiency will go to 0 as Q goes to 0.

= Three: Semaphores =
Suppose we have two threads Ta and Tb that we would like to wait for each other at a certain point in their execution

=== Thread Ta ===
{{{
statement a1
rendezvous_a()
statement a2
}}}
=== Thread Tb ===
{{{
statement b1
rendezvous_b()
statement b2
}}}

Here, we want Ta not to execute a2 until Tb has executed b1 and we want Tb not to execute b2 until Ta has executed a1. That is, we want the threads to "rendezvous" by calling rendezvous_a() and rendezvous_b().

Give a semaphore-based solution for rendezvous_a() and rendezvous_b(). Declare any variables you might need, along with their initial values. Make sure that deadlock is not possible in your solution.

== Answer ==

So, just to be clear, these orders of statements are OK:

a1, b1, a2, b2

b1, a1, a2, b2

a1, b1, b2, a2

b1, a1, b2, a2

But these aren't:

a1, a2, b1, b2

a1, a2, b2, b1

b1, b2, a1, a2

b1, b2, a2, a1

a1 and b1 both have to finish before either a2 or b2 may run.  I think we should be able to do this:

{{{
int sem = 2;

void rendezvous_a() {
sem--;
while(sem > 0) { wait(); }
}

void rendezvous_b() {
sem--;
while(sem > 0) { wait(); }
}
}}}

where wait() is just a function that sleeps that thread for an appropriate amount of time.

Off the top of my head, I don't see deadlock in this.  Neither `rendezvous` function can finish unless the other one is called, and that can't happen unless we're past the statement we cared about having happen.  One concern might be that the `wait()`ing doesn't allow enough time for the other thread to run, so we'd have to make sure to have the waiting period be long enough.