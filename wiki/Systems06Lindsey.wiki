#summary Lindsey studies for the systems qual.

= One: Threads =
== Which of the following items are typically considered to be unique to a process, but not unique to a thread? ==
  * CPU registers
  * page table pointer
  * stack pointer
  * open files
  * segment table
  * child processes
  * program counter

== Answer ==

CPU registers: These are unique to a thread; they're part of the context that gets saved when we swap a thread out.

Page table pointer: There is only one, OS-wide page table, so it wouldn't make sense for each thread to have a pointer into the page table; this would be a process-level notion if not an OS-level notion.

Stack pointer and program counter: These would have to be unique to a thread; threads have to keep track of where they are in execution.

Open files: Unique to a process, not a thread; two threads in a process might both access a file, even though only one of them opened it.

Segment table: This keeps track of allocated and available memory in a process' address space (for instance, the information in /proc/pid/maps for a particular pid on Linux), so it's process-specific, not thread-specific.

Child processes: An individual thread can't fork a child process; it has to be done at the process level.  So, unique to a process, not a thread.

So, in all, we've got:

  * CPU registers -- thread
  * page table pointer -- OS
  * stack pointer -- thread
  * open files -- process
  * segment table -- process
  * child processes -- process
  * program counter -- thread

On page 102 of Modern Operating Systems 3e, we have:

|| *Per process items*         || *Per thread items* ||
|| Address space               || Program counter    ||
|| Global variables            || Registers          ||
|| Open files                  || Stack              ||
|| Child processes             || State              ||
|| Pending alarms              ||                    ||
|| Signals and signal handlers ||                    ||
|| Accounting information      ||                    ||

I'm right about open files, child processes, program counter, CPU registers, stack pointer (I assume that is part of "stack").  "Address space" could possibly count as "segment table", so that's OK too.  No word about "page table pointer" -- I still think that that's OS-wide.

== Which of the following are true statements about user-level threads versus kernel-level threads? ==

  * Invoking any system call that might block poses an extra problem for user-level threads
  * User level threads are more vulnerable to priority inversion than kernel-level threads
  * There is less overall scheduling overhead in a user-level thread system
  * User level threads retain advantages on symmetric multiprocessors
  * Upcalls are necessary to implement user-level threads. (An upcall is a notification, provided by the kernel to the user-level thread run-time system, that a thread has blocked.)

== Answer == 

From Tanenbaum, p. 108 and thereabouts:

"Invoking any system call that might block poses an extra problem for user-level threads" -- true.  If there are several user-level threads in a process and one blocks on a system call, none of them can run because as far as the kernel is concerned, the entire process is blocking.  If the kernel is aware of threads it might be able to allow those threads not blocking on the system call to run.

"User level threads are more vulnerable to priority inversion than kernel-level threads" -- false.  I don't see any particular reason why this would be the case.

"There is less overall scheduling overhead in a user-level thread system" -- true, because there is no need to go into the kernel when a thread is finished running in order to pick another thread to run; thread scheduling and context switching can happen without calling int the kernel. 

"User level threads retain advantages on symmetric multiprocessors" -- false; I don't see any particular reason why this would be the case, either.

"Upcalls are necessary to implement user-level threads." -- I would say false, that upcalls are not _required_, although they might help with scheduling.

= Two: CPU Efficiency =
Measurements of a certain system have shown that the average process runs for a time T before blocking on I/O. A process switch requires time S, which is effectively wasted (over-head). For round-robin scheduling with quantum Q, give a formula for the CPU efficiency for each of the following.
  * Q = infinity
  * Q > T
  * S < Q < T
  * Q = S
  * Q nearly 0

== Answer ==

Is the "CPU efficiency" the percentage of time spent actually computing something?  Let's assume that that's the case.

Off the top of my head, I thought that round-robin scheduling with quantum Q meant that each process got time Q, whether it needed it or not; if it finished during its quantum, the CPU would be idle until the end of the quantum.  Some googling (this question shows up [http://www.google.com/search?q=Measurement+of+a+certain+system+have+shown+that+the+average+process+runs+for+a+time+T+before+blocking+on+IO. all over]) suggests that that's not the case, and Tanenbaum (p. 154) confirms it: "Each process is assigned a time interval, called its quantum, during which it is allowed to run.  If the process is still running at the end of the quantum, the CPU is preempted and given to another process.  If the process has blocked or finished before the quantum has elapsed, the CPU switching is done when the process blocks, of course."

I don't know if we should include the context switch time S in Q, or if it's in addition to Q, but let's assume that it's in addition to Q.

Q = infinity: The process will have the CPU until it blocks.  So, the process will run for T; then the next process will run for T (plus the S needed for the switch), and so on.  Each process will need T+S of processor time to get T worth of work done, so the efficiency is T/(T+S).  (Pretty good for CPU efficiency, but CPU efficiency may not be the only thing we want to maximize!)

Q > T: If the quantum is large enough that processes always block on I/O before it is over, the efficiency will be the same as in the "infinity" example: T/(T+S).

S < Q < T: Here, processes don't finish running during their quantum.  We don't know how big of a difference there is between Q and T, but we know that the process will require T/Q process switches during its lifetime, and each of those will take time S, so, S `*` (T/Q) time will be needed for context switching.  In all, we will need T + (S `*` (T/Q)) time to get work done that should take time T if it were allowed to happen all at once, so the efficiency is: T / (T + (ST/Q)).

Some clever people have simplified this as follows:

{{{
T            T              T              T              1          Q
--------  =  ----------- =  ----------- =  ----------- =  ------- =  ---
T + ST/Q     TQ/Q + ST/Q    (TQ + ST)/Q    T * (Q+S)/Q    (Q+S)/Q    Q+S
}}}

So, Q/(Q+S).

Q = S: Again, a process will need to do T/Q context switches in its lifetime, and if Q is S, that's equal to T/S.  It'll need to do that S times.  So, a total of T time spent context switching.  In all, we'll need T time to do work plus T time spent context switching, so, T /(T+T), or 50% efficiency.

Q = nearly 0: We won't even have time to do the context switching -- efficiency will go to 0 as Q goes to 0.

= Three: Semaphores =
Suppose we have two threads Ta and Tb that we would like to wait for each other at a certain point in their execution

=== Thread Ta ===
{{{
statement a1
rendezvous_a()
statement a2
}}}
=== Thread Tb ===
{{{
statement b1
rendezvous_b()
statement b2
}}}

Here, we want Ta not to execute a2 until Tb has executed b1 and we want Tb not to execute b2 until Ta has executed a1. That is, we want the threads to "rendezvous" by calling rendezvous_a() and rendezvous_b().

Give a semaphore-based solution for rendezvous_a() and rendezvous_b(). Declare any variables you might need, along with their initial values. Make sure that deadlock is not possible in your solution.

== Answer ==

So, just to be clear, these orders of statements are OK:

a1, b1, a2, b2

b1, a1, a2, b2

a1, b1, b2, a2

b1, a1, b2, a2

But these aren't:

a1, a2, b1, b2

a1, a2, b2, b1

b1, b2, a1, a2

b1, b2, a2, a1

a1 and b1 both have to finish before either a2 or b2 may run.  I think we should be able to do this:

{{{
int sem = 2;

void rendezvous_a() {
sem--;
while(sem > 0) { wait(); }
}

void rendezvous_b() {
sem--;
while(sem > 0) { wait(); }
}
}}}

where wait() is just a function that sleeps that thread for an appropriate amount of time.

Off the top of my head, I don't see deadlock in this.  Neither `rendezvous` function can finish unless the other one is called, and that can't happen unless we're past the statement we cared about having happen.  One concern might be that the `wait()`ing doesn't allow enough time for the other thread to run, so we'd have to make sure to have the waiting period be long enough.

*TODO: read about deadlock -- see if I'm overlooking anything here.*

= Four: Deadlock =
Consider the following program with two concurrent processes foo and bar:

{{{
int x = 0, y = 0, z = 0;
sem lock1 = 1, lock2 = 1;

process foo {
  z = z + 2;
  P(lock1);
  x = x + 2;
  P(lock2);
  V(lock1);
  y = y + 2;
  V(lock2);
}

process bar {
  P(lock2);
  y = y + 1;
  P(lock1);
  x = x + 1;
  V(lock1);
  V(lock2);
  z = z + 1;
}
}}}

Here the variables lock1 and lock2 are semaphores and the P() and V() functions are the associated "down" and "up" operations, respectively.

  * How might this program deadlock? You should demonstrate that the conditions you list specifically meet all of the conditions required for deadlock to occur.
  * Can the program deadlock in more than one state?
  * What are the possible final values of x, y, and z in the deadlock state? If the program can deadlock in more than one state, please specify which state results in the final values you give.
  * What are the possible final values of x, y, and z if the program terminates?

== Answer == 

*TODO later!*

= Five: Virtual Memory =

Consider a system that uses 32-bit address and 3-level page table. Among the 32-bit virtual addresses, 4 bits are used to index into the first-level page table, 8 bits for the second-level, and another 8 bits for the third-level.
  * What is the page size of this system?
  * If a page table entry is 2 bytes long, what is the size of the second-level page table?
  * Give reasons why the page size in a virtual memory system should neither be too large or too small.
  * What is a translation lookaside buffer (TLB) and what does it do?

== Answer == 

First, "what is the page size of this system?"  First let's talk about how the 3-level page table works.  We have 32-bit addresses.  In a given address, we're told that 4 of those bits index into the first-level page table, which means that there can be 16 distinct values, so the first-level page table has 16 rows, each of which point us into one of 16 second-level tables.

Then, each address has 8 bits that index into a second-level page table, so that's 256 values.  So the second-level table has 256 rows, each of which points us into one of 256 third-level tables.  Finally, each address has 8 bits that index into a third-level page table with 256 rows, each of which points us to a particular page.  Subtracting the 4, 8, and 8, there are 12 bits left in the virtual address, so 12 bits left to determine location on that page.  2^12 = 4096, so there are 4096 locations on the page.  If each location is a byte (that is, if memory is byte-addressable in this system), then the page size is 4KB.  If memory is word-addressable with, say, 32-bit words (aka 4-byte words), then the page size is 4 times that, 16KB.

Second, "If a page table entry is 2 bytes long, what is the size of the second-level page table?"

Well, we decided that there are really 16 separate second-level tables, so let's figure out what the size of one is and then multiply it by 16.  The table has 256 rows, so 256 `*` 2 bytes = 512 bytes.  Then, multiplying 512 bytes by 16, we get 8192 bytes or 8KB.  Each of the 16 will probably have some constant amount of header information, though, so in reality it will be a little larger.

Third, "Give reasons why the page size in a virtual memory system should neither be too large or too small."

The appropriate page size depends on the nature and locality of the data that needs to be brought into memory.  We can only bring data into memory in increments of a page, so if we have large pages and we only need a few bytes of data from each page we bring into memory, then all of the other space that those pages take up in memory will be wasted as far as we're concerned.  On the other hand, if we have small pages and we need lots of data that's consecutive in memory, then it would be more efficient to bring it in on large pages together and avoid the per-page overhead of having to page in lots of small pages.

Fourth, "What is a translation lookaside buffer (TLB) and what does it do?"

(With some help from Wikipedia:) A TLB is typically a hardware cache adjacent to the CPU that speeds the process of translating virtual addresses to physical addresses.  When an address needs to be translated, the TLB's mapping of virtual to physical addresses is searched first.  If the TLB doesn't have a mapping for a particular virtual address, we have to do a page table lookup, or page walk, of that address, which is expensive, especially in the case of a multi-level page table like this one, since we have to read several memory locations in order to compute the physical address.  After the address has been computed, it's saved in the TLB so it will be available next time it's needed.

= Six: Caches =
Consider a program that accesses pages in the following order:
{{{
1,3,4,1,2,6,1,1,3,2,4,5,4,2,3,2,4
}}}

Assuming no pages are in memory when the program starts, show how many page faults will occur when using the following replacement algorithms and number of frames.
  * LRU, one frame
  * FIFO, one frame
  * LRU, three frames
  * FIFO, three frames

== Answer == 

From what I can tell on [http://en.wikipedia.org/wiki/Page_replacement_algorithm the Wikipedia page for "page replacement algorithm"], a frame is simply a slot in memory that can hold a page.  Working on that assumption:

LRU, one frame: There's only room for one page in memory.  We'll initially get a page fault when trying to access page 1, and we'll page in page 1.  Then we'll get a page fault trying to access page 3, and page it in, and so on.  We'll get a page fault every time we try to access any page, unless it was the same as the previous one accessed.  So, for the 17 page accesses, we'll get 16 page faults (all except the repeated access of page 1).

FIFO, one frame: It will be the same as above; there's never a choice of what to page out since there's only one frame, so the choice of page replacement algorithm doesn't matter.

LRU, three frames: Now the choice of page replacement algorithm starts to matter.  The accesses will go like this:

{{{
1 -- page fault; page in 1.
3 -- page fault; page in 3.
4 -- page fault; page in 4.
4 -- already there.
1 -- already there.
2 -- page fault; page out 3 since it's least recently accessed of {1, 3, 4} and page in 2.  Now memory contains {1, 4, 2}.
6 -- page fault; page out 4 and page in 6.  Now memory contains {1, 2, 6}.
1 -- already there.
1 -- already there.
3 -- page fault; page out 2 and page in 3.  Now memory contains {1, 6, 3}.
2 -- page fault; page out 6 and page in 2.  Now memory contains {1, 3, 2}.
4 -- page fault; page out 1 and page in 4.  Now memory contains {3, 2, 4}.
5 -- page fault; page out 3 and page in 5.  Now memory contains {2, 4, 5}.
4 -- already there.
2 -- already there.
3 -- page fault; page out 5 and page in 3.  Now memory contains {2, 4, 3}.
2 -- already there.
4 -- already there.
}}}

Whew.  still pretty bad, but this time we only had 10 page faults for 17 accesses.

FIFO, three frames:

{{{
1 -- page fault; page in 1.
3 -- page fault; page in 3.
4 -- page fault; page in 4.
4 -- already there.
1 -- already there.
2 -- page fault; page out 1 since it was first in and page in 2.  Memory: {3, 4, 2}.
6 -- page fault; page out 3 and page in 6.  Memory: {4, 2, 6}.
1 -- page fault; page out 4 and page in 1.  Memory: {2, 6, 1}.
1 -- already there.
3 -- page fault; page out 2 and page in 3.  Memory: {6, 1, 3}.
2 -- page fault; page out 6 and page in 2.  Memory: {1, 3, 2}.
4 -- page fault; page out 1 and page in 4.  Memory: {3, 2, 4}. 
5 -- page fault; page out 3 and page in 5.  Memory: {2, 4, 5}.
4 -- already there.
2 -- already there.
3 -- page fault; page out 2 and page in 3.  Memory: {4, 5, 3}.
2 -- page fault; page out 4 and page in 2.  Memory: {5, 3, 2}.
4 -- already there.
}}}

That time we got twelve page faults out of 17 page accesses, so, not quite as good as LRU.

= Seven: Where's the Bottleneck? =
While running a job, the CPU utilization ratio is 15%, the paging-disk utilization is 90%, and the network is used at 40% of its capacity. Please identify which of the following may improve the overall throughput of the program and explain why.
  * Install a faster CPU
  * Double the size of the memory
  * Double the network bandwidth
  * Double the size of the disk
  * Upgrade the paging algorithm to include pre-fetching
  * Increase the page size

== Answer == 

I'm going to assume that "paging-disk utilization" is the amount of swap space that's being used.  It looks like this system is spending most of its time swapping pages in and out.  Of the suggested ideas, doubling the memory is most likely to help, because we'd have to swap less often.  Increasing the page size and upgrading the paging algorithm to use pre-fetching of pages could either hurt or help, depending on the locality of the data.

Installing a faster CPU, increasing the network bandwidth, and doubling the size of the disk are not likely to help: a faster CPU might help if the CPU were pegged, but since CPU utilization is only 15%, it won't improve throughput.  Similarly, increasing the network bandwidth won't improve throughput since network utilization is only 40%.  Doubling the size of the disk might actually hurt if it makes the disk slower.

= Eight: Filesystems =
Consider a "standard" UNIX filesystem (with i-nodes, indirect blocks, etc). Assume that the file block cache is initially empty, and that the i-node cache initially contains the current working directory. Assume "bar" exists in the current working directory.

Suppose a program executes the following sequence of instructions.
{{{
fd = Open("bar"); // open the file
Seek(fd,0); // seek to beginning of file
Write(fd, "A", 1); // write one byte
Close(fd); // close the file
}}}

Reading or writing one block or i-node requires 1 disk access.
  * What is the minimum number of disk accesses that this sequence of instructions could possibly require? If it is not possible to give an exact value, explain why and indicate what additional information is needed.
  * What is the maximum number of disk accesses that this sequence of instructions could possibly require? If it is not possible to give an exact value, explain why and indicate what additional information is needed.

== Answer == 

*TODO later!*

= Nine: System  Calls=
To access an operating system service, a user process must make a system call. In a user program, the interface presented by a system call is a library procedure with the same name as the system call, eg, read(), which is prototyped as follows:
{{{
ssize_t read(int fd, void *buf, size_t count);
}}}

  * An operating system exists to provide a number of services and functions to users. What needs are met by having system calls?
  * What are the functional differences between a system call and a normal (non-system) function call? IE, what things must you do with a system call (and why) that you can't do with a normal function call?
  * What are the operational differences between a system call and a normal (non-system) function call? IE, what are some differences in how they are implemented?
  * What are the precise series of steps performed when a system call is invoked? If you like, you may illustrate your answer using the read() system call shown above (or you can use a system call of your choice).

== Answer == 

*TODO later!*