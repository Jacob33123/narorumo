#summary intro to information retrieval

This is the notes towards a talk to be given for Spelman College's CIS313, 24 September 2008.

My source material here is the excellent  Stuart Russell and Peter Norvig's
_Artificial Intelligence: A Modern Approach_. (http://aima.cs.berkeley.edu/)

Or wikipedia articles, which I'll list here too.

== What's Information Retrieval? ==

How do we define the task?

How do you store/represent your copy of the corpus? What information do you
want to extract about your documents?

== first pass ==
Logical connectives and your query language

Russell and Norvig say:
  After reading the previous chapter, one might suppose that an information
  retrieval system could be built by parsing the document collection into a
  knowledge base of logical sentences and then parsing each query and `ASK`ing
  the knowledge base for answers. Unfortunately, no one has ever succeeded in
  building a large-scale IR system that way. It is just too difficult to build
  a lexicon and grammar that cover a large document collection, so all IR
  systems use simpler language models.

Alright, let's go for a simpler language model.

=== Transition model ===
Instead of trying to parse out and understand all the documents in the corpus, we can build a probabilistic model out of the sequences of words that we might find.

We could use bigrams, or trigrams.

=== Vector space model ===
http://en.wikipedia.org/wiki/Vector_space_model

You'll also hear the term "bag of words" model, where a bag is like a multiset. You just count how many times each word appears in a document. If a document contains the word "waffle", then it's probably about waffles. Sounds good!

*example!*

What are some problems with this?
- Some words show up an awful lot.
- This comes in two flavors: some words aren't helpful for this task at all. Language processing people refer to these as "stopwords", and typically you'll want to take them out immediately. More subtly, some words show up very rarely, but if they do show up, you almost certainly want to take note.

One thing we could do, we could just filter out all the "stopwords".
http://en.wikipedia.org/wiki/Stop_words

Okay, but maybe we can do better than that. Look at all of this stuff about Spain and France, in the Andorra article.

=== tf-idf ===
A fairly standard way to handle this is with what's called TF-IDF, Term-Frequency, Inverse Document Frequency.

http://en.wikipedia.org/wiki/Tf-idf

Term frequency is just the proportion of the words in the document that are this term.

Inverse document frequency is more involved -- it's higher for words that occur in the whole corpus less often.

- Polysemy: some words mean more than one thing. "apple" like computers, "apple" like fruit.
- synonyms: harder, there's more than one word for a given topic! How do we make this fall out of the numbers?

== How do you make it fast? ==
Parallelism? MapReduce?

== Ranking ==
So once we've 

=== PageRank ===
PageRank!

== Questions? ==